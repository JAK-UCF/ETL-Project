{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELT Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Data:\n",
    "\n",
    "DATA SOURCE: https://www.kaggle.com/datasnaek/youtube-new/data <br/>\n",
    "Utilizing: <br/>\n",
    "3 csv files with Video Information (Canada, US, and Britain) <br/>\n",
    "3 json files with Category Assignment (Canada, US, and Britain) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup & Analysis\n",
    "\n",
    "Plan and document the following:\n",
    "* The sources of data that you will extract from.\n",
    "* The type of transformation needed for this data (cleaning, joining, filtering, aggregating, etc).\n",
    "* The type of final production database to load the data into (relational or non-relational).\n",
    "* The final tables or collections that will be used in the production database.\n",
    "\n",
    "You will be required to submit a final technical report with the above information and steps required to reproduce your ETL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Report:\n",
    "\n",
    "Submit a Final Report that describes the following:\n",
    "* Extract: your original data sources and how the data was formatted (CSV, JSON, pgAdmin 4, etc).\n",
    "* Transform: what data cleaning or transformation was required.\n",
    "* Load: the final database, tables/collections, and why this was chosen.\n",
    "\n",
    "Please upload the report to Github and submit a link to Bootcampspot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "# pd.options.display.max_rows = 3000\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in Canada csv video info\n",
    "ca_file = os.path.join(\"data\", \"CAvideos.csv\")\n",
    "CA_df = pd.read_csv(ca_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in Canada json category keys\n",
    "json_CA = os.path.join(\"data\", \"CA_category_id.json\")\n",
    "category_CA_df = pd.read_json(json_CA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in Great Britain csv video info\n",
    "gb_file = os.path.join(\"data\", \"GBvideos.csv\")\n",
    "GB_df = pd.read_csv(gb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in Great Britain json category keys\n",
    "json_GB = os.path.join(\"data\", \"GB_category_id.json\")\n",
    "category_GB_df = pd.read_json(json_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in United States csv video info\n",
    "us_file = os.path.join(\"data\", \"USvideos.csv\")\n",
    "US_df = pd.read_csv(us_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in United States json category keys\n",
    "json_US = os.path.join(\"data\", \"US_category_id.json\")\n",
    "category_US_df = pd.read_json(json_US)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up Canada category df with json_normalize (pulls dictionary items into their own column)\n",
    "#drop static YouTube info not needed for video database, rename column and cast category_id to number for merge\n",
    "CA_category_df = json_normalize(category_CA_df['items'])\n",
    "CA_category_df.drop(['etag', 'kind', 'snippet.assignable', 'snippet.channelId'], axis=1, inplace=True)\n",
    "CA_category_df.rename(columns={'id': 'category_id', 'snippet.title': 'category_name'}, inplace=True)\n",
    "CA_categories = CA_category_df.astype({'category_id': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column to Canada video df to define which country info came from after upcoming concat\n",
    "CA_df.insert(1,\"country\", \"CA\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge category_names into CA_df\n",
    "CAfull_df = CA_df.merge(CA_categories, how='left', on=\"category_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAfull_df.count()     # video_id: 40881 - category_name: 40807  [NaN: 74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up Great Britain category df with json_normalize (pulls dictionary items into their own column)\n",
    "#drop static YouTube info not needed for video database, rename column and cast category_id to number for merge\n",
    "GB_category_df = json_normalize(category_GB_df['items'])\n",
    "GB_category_df.drop(['etag', 'kind', 'snippet.assignable', 'snippet.channelId'], axis=1, inplace=True)\n",
    "GB_category_df.rename(columns={'id': 'category_id', 'snippet.title': 'category_name'}, inplace=True)\n",
    "GB_categories = GB_category_df.astype({'category_id': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column to Great Britain video df to define which country info came from after upcoming concat\n",
    "GB_df.insert(1,\"country\", \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge category_names into GB_df\n",
    "GBfull_df = GB_df.merge(GB_categories, how='left', on=\"category_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBfull_df.count()     # video_id: 38916 - category_name: 38826  [NaN: 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up United States category df with json_normalize (pulls dictionary items into their own column)\n",
    "#drop static YouTube info not needed for video database, rename column and cast category_id to number for merge\n",
    "US_category_df = json_normalize(category_US_df['items'])\n",
    "US_category_df.drop(['etag', 'kind', 'snippet.assignable', 'snippet.channelId'], axis=1, inplace=True)\n",
    "US_category_df.rename(columns={'id': 'category_id', 'snippet.title': 'category_name'}, inplace=True)\n",
    "US_categories = US_category_df.astype({'category_id': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column to United States video df to define which country info came from after upcoming concat\n",
    "US_df.insert(1,\"country\", \"US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge category_names into US_df\n",
    "USfull_df = US_df.merge(US_categories, how='left', on=\"category_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USfull_df.count()     # video_id: 40949 - category_name: 40949  [NaN: 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull only the most recent stats per video (for each CA, GB, and US df)\n",
    "#1> add MaxDate col: take most recent trending_date for each video_id and assign every copy of that video_id\n",
    "#2> filter the df to only the videos where trending_date and MaxDate are the same\n",
    "\n",
    "CAfull_df['MaxDate'] = CAfull_df.groupby('video_id').trending_date.transform('max') # CA_df COUNT: 40881(max)\n",
    "final_CA_df = CAfull_df[CAfull_df['MaxDate'] == CAfull_df['trending_date']] # final_CA_df COUNT: 24427(max)\n",
    "\n",
    "GBfull_df['MaxDate'] = GBfull_df.groupby('video_id').trending_date.transform('max') # GB_df COUNT: 38916(max)\n",
    "final_GB_df = GBfull_df[GBfull_df['MaxDate'] == GBfull_df['trending_date']] # final_CA_df COUNT: 3300(max)\n",
    "\n",
    "USfull_df['MaxDate'] = USfull_df.groupby('video_id').trending_date.transform('max') # US_df COUNT: 40949(max)\n",
    "final_US_df = USfull_df[USfull_df['MaxDate'] == USfull_df['trending_date']] # final_US_df COUNT: 6354(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_frame = pd.concat([final_CA_df, final_GB_df, final_US_df], ignore_index=True) # full_frame COUNT: 34081(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL NaN VALUES: 'description' [filling 1116 NaN descriptions]\n",
    "full_frame['description'].fillna(\"No description provided.\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_frame.count()     # video_id: 34081 - category_name: 34026  [NaN: 55 (others were dropped in the MaxDate filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL NaN VALUES: 'category_name'\n",
    "# unsure why none of the json sets contained the category assigner for 'Nonprofits & Activism' (category_id 29 *Google)\n",
    "# specifying this category_id for future use (if category_id != 29, will not fillna...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index list of all category_id == 29 rows\n",
    "\n",
    "# cat_id_29_idx = full_frame.index[full_frame['category_id'] == 29]\n",
    "# cat_id_29_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test frame based on category_id == 29\n",
    "#run before and after setting np.array as series to replace NaN in category_name with 'Nonprofits...'\n",
    "\n",
    "# validator = full_frame.loc[cat_id_29_idx]\n",
    "# validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaN 'Nonprofits...' in an array\n",
    "cat_name_full_set = np.where(pd.isnull(full_frame.category_name), \"Nonprofits & Activism\", full_frame.category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert array to pandas series and overwrite 'category_name' column\n",
    "full_frame['category_name'] = pd.Series(cat_name_full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 'publish_time' to datetime format\n",
    "full_frame['publish_time'] = pd.to_datetime(full_frame['publish_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull just date from full publish_date\n",
    "full_frame['publish_date'] = full_frame['publish_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns not using for project\n",
    "full_frame.drop(['thumbnail_link', \n",
    "                 'ratings_disabled', \n",
    "                 'video_error_or_removed', \n",
    "                 'publish_time', \n",
    "                 'MaxDate'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorganize columns\n",
    "full_frame = full_frame[['video_id',\n",
    "                         'title', \n",
    "                         'channel_title', \n",
    "                         'views', \n",
    "                         'likes', \n",
    "                         'dislikes', \n",
    "                         'comments_disabled', \n",
    "                         'comment_count', \n",
    "                         'description', \n",
    "                         'tags', \n",
    "                         'category_id', \n",
    "                         'category_name', \n",
    "                         'publish_date', \n",
    "                         'trending_date', \n",
    "                         'country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0yIWz1XEeyc</td>\n",
       "      <td>Jake Paul Says Alissa Violet CHEATED with LOGA...</td>\n",
       "      <td>DramaAlert</td>\n",
       "      <td>1309699</td>\n",
       "      <td>103755</td>\n",
       "      <td>4613</td>\n",
       "      <td>False</td>\n",
       "      <td>12143</td>\n",
       "      <td>► Follow for News! - https://twitter.com/KEEMS...</td>\n",
       "      <td>#DramaAlert|\"Drama\"|\"Alert\"|\"DramaAlert\"|\"keem...</td>\n",
       "      <td>25</td>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FyZMnhUtLfE</td>\n",
       "      <td>猎场 | Game Of Hunting 12【TV版】（胡歌、張嘉譯、祖峰等主演）</td>\n",
       "      <td>大劇獨播</td>\n",
       "      <td>158815</td>\n",
       "      <td>218</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>186</td>\n",
       "      <td>Thanks for watching the drama! Help more peopl...</td>\n",
       "      <td>電視劇|\"大陸電視劇\"|\"猎场\"|\"职场\"|\"商战\"|\"爱情\"|\"都市\"|\"胡歌\"|\"陈龙\"...</td>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>2017-11-12</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7MxiQ4v0EnE</td>\n",
       "      <td>Daang ( Full Video ) | Mankirt Aulakh | Sukh S...</td>\n",
       "      <td>Speed Records</td>\n",
       "      <td>5718766</td>\n",
       "      <td>127477</td>\n",
       "      <td>7134</td>\n",
       "      <td>False</td>\n",
       "      <td>8063</td>\n",
       "      <td>Song - Daang\\nSinger - Mankirt Aulakh\\nFaceboo...</td>\n",
       "      <td>punjabi songs|\"punjabi bhangra\"|\"punjabi music...</td>\n",
       "      <td>10</td>\n",
       "      <td>Music</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gifPYwArCVQ</td>\n",
       "      <td>Fake Pet Smart Employee Prank!</td>\n",
       "      <td>NELK</td>\n",
       "      <td>557883</td>\n",
       "      <td>44558</td>\n",
       "      <td>621</td>\n",
       "      <td>False</td>\n",
       "      <td>9619</td>\n",
       "      <td>3 Days left to cop NELK merch: https://nelk.ca...</td>\n",
       "      <td>prank|\"pranks\"|\"nelk\"|\"nelkfilmz\"|\"nelkfilms\"</td>\n",
       "      <td>23</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8NHA23f7LvU</td>\n",
       "      <td>Jason Momoa Wows Hugh Grant With Some Dothraki...</td>\n",
       "      <td>The Graham Norton Show</td>\n",
       "      <td>1496225</td>\n",
       "      <td>16116</td>\n",
       "      <td>236</td>\n",
       "      <td>False</td>\n",
       "      <td>605</td>\n",
       "      <td>I think Sarah Millican was very excited for th...</td>\n",
       "      <td>Graham Norton|\"Graham Norton Show Official\"|\"E...</td>\n",
       "      <td>24</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  0yIWz1XEeyc  Jake Paul Says Alissa Violet CHEATED with LOGA...   \n",
       "1  FyZMnhUtLfE         猎场 | Game Of Hunting 12【TV版】（胡歌、張嘉譯、祖峰等主演）   \n",
       "2  7MxiQ4v0EnE  Daang ( Full Video ) | Mankirt Aulakh | Sukh S...   \n",
       "3  gifPYwArCVQ                     Fake Pet Smart Employee Prank!   \n",
       "4  8NHA23f7LvU  Jason Momoa Wows Hugh Grant With Some Dothraki...   \n",
       "\n",
       "            channel_title    views   likes  dislikes  comments_disabled  \\\n",
       "0              DramaAlert  1309699  103755      4613              False   \n",
       "1                    大劇獨播   158815     218        30              False   \n",
       "2           Speed Records  5718766  127477      7134              False   \n",
       "3                    NELK   557883   44558       621              False   \n",
       "4  The Graham Norton Show  1496225   16116       236              False   \n",
       "\n",
       "   comment_count                                        description  \\\n",
       "0          12143  ► Follow for News! - https://twitter.com/KEEMS...   \n",
       "1            186  Thanks for watching the drama! Help more peopl...   \n",
       "2           8063  Song - Daang\\nSinger - Mankirt Aulakh\\nFaceboo...   \n",
       "3           9619  3 Days left to cop NELK merch: https://nelk.ca...   \n",
       "4            605  I think Sarah Millican was very excited for th...   \n",
       "\n",
       "                                                tags  category_id  \\\n",
       "0  #DramaAlert|\"Drama\"|\"Alert\"|\"DramaAlert\"|\"keem...           25   \n",
       "1  電視劇|\"大陸電視劇\"|\"猎场\"|\"职场\"|\"商战\"|\"爱情\"|\"都市\"|\"胡歌\"|\"陈龙\"...            1   \n",
       "2  punjabi songs|\"punjabi bhangra\"|\"punjabi music...           10   \n",
       "3      prank|\"pranks\"|\"nelk\"|\"nelkfilmz\"|\"nelkfilms\"           23   \n",
       "4  Graham Norton|\"Graham Norton Show Official\"|\"E...           24   \n",
       "\n",
       "      category_name publish_date trending_date country  \n",
       "0   News & Politics   2017-11-13      17.14.11      CA  \n",
       "1  Film & Animation   2017-11-12      17.14.11      CA  \n",
       "2             Music   2017-11-11      17.14.11      CA  \n",
       "3            Comedy   2017-11-13      17.14.11      CA  \n",
       "4     Entertainment   2017-11-10      17.14.11      CA  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
